<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Khurram Azeem Hashmi</title>
    <meta name="description" content="Academic website of Khurram Azeem Hashmi">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@400;500&display=swap">
    <link rel="stylesheet" href="style.css">
    <link rel="icon" type="image/png" href="icons/favicon.png">
  </head>
  <body>
    <div class="menu">
      <a href="#home">Home</a> &nbsp;&nbsp;&nbsp;
      <a href="#bio">Bio</a> &nbsp;&nbsp;&nbsp;
      <a href="#projects">Projects</a> &nbsp;&nbsp;&nbsp;
      <a href="#publications">Publications</a> &nbsp;&nbsp;&nbsp;
      <a href="#services">Services/Honors</a>
    </div>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="home">
              <td style="padding:20px;width:67%;vertical-align:middle">
                <span class="name">Khurram Azeem Hashmi</span>
                <p class="information">
                  Machine Learning Engineer at <a href="https://av.dfki.de/">DFKI</a>
                  and Ph.D. Candidate at <a href="https://rptu.de/">RPTU Kaiserslautern</a>
                </p>
                <p class="information">
                  <a href="mailto:khurram.tukl@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=0CN11t4AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/khurramhashmi3">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/khurramHashmi">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/khurramhashmi/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:20px;width:33%;vertical-align:middle">
                <img class="dp" src="figures/me.jpg" alt="Profile photo">
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="bio">
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="section">About Me</span>
                <p class="bio">
                  I am a Machine Learning Engineer at the German Research Center for Artificial Intelligence <a href="https://dfki.de/">(DFKI)</a> and a PhD Candidate at RPTU Kaiserlautern-Landau, working under the supervision of Prof. Dr. Didier Stricker. My research focuses on advancing computer vision and deep learning, particularly in developing robust solutions for image and video understanding in challenging environments with limited spatial or labeled information. Recently nominated for the AI Person of the Year Award 2024, my work has been published at top-tier conferences including CVPR, ICCV, and WACV.
                </p>
                <p class="bio">
                  My current work spans several exciting areas: developing sparse transformers for efficient scene understanding, enhancing visual perception in challenging environments, and creating multi-modal systems for robotic applications. I lead projects like HERON, which focuses on vision-guided robotic assembly, and work on open-vocabulary scene understanding for ego-centric perception. My approach combines theoretical innovation with practical applications, aiming to bridge the gap between academic research and real-world deployment. I'm particularly passionate about making AI systems more efficient, interpretable, and applicable to real-world scenarios, with a current focus on exploring novel trends in Agentic AI and autonomous learning systems.
                </p>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="projects">
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="section">Projects</span>
                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/heron.gif" alt="HERON">
                      </td>
                      <td>
                        <papertitle>HERON: Multi-Modal Vision Guided Navigation System for Collaborative Assembly Robots</papertitle>
                        A multi-modal vision guided navigation system designed for collaborative assembly robots, featuring advanced capabilities in obstacle avoidance, path planning, and precise screw mounting operations. The system integrates multiple sensor modalities to ensure robust and efficient robot-human collaboration in assembly tasks.
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/luminous.gif" alt="Open Vocabulary Egocentric Scene Understanding">
                        <br>
                        <em><b>Prompt:</b> Make a Pasta</em>
                      </td>
                      <td>
                        <papertitle>Open Vocabulary Egocentric Scene Understanding</papertitle>
                        This project leverages multi-modal foundation models to enhance egocentric scene understanding in unconstrained environments. The system enables natural language-driven task execution through LLM prompting, allowing users to perform complex sequences of actions by simply describing tasks like <i>make a pasta</i>. This approach combines advanced vision-language models with practical augmented reality applications.
                      </td>
                    </tr>
                  </table>
                </div>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="publications">
              <td style="padding:0 20px 20px 20px;width:100%;vertical-align:middle">
                <span class="section">Selected Publications <a href="https://scholar.google.com/citations?user=0CN11t4AAAAJ&hl=en">[Google Scholar]</a></span>
                <div class="paper" style="background-color: #ffffd0">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/vod_mot.gif" alt="Beyond Boxes">
                      </td>
                      <td>
                        <papertitle>Beyond Boxes: Mask-Guided Spatio-Temporal Feature Aggregation for Video Object Detection</papertitle>
                        <strong>K.A. Hashmi</strong>, T.U. Sheikh, D.Stricker, M.Z. Afzal
                        <br>
                        <em>WACV</em>, 2025
                        <br>
                        <a href="https://arxiv.org/pdf/2412.04915">pdf</a> /
                        webpage (coming soon) /
                        code (coming soon)
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper" style="background-color: #ffffd0">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/sparse_semidetr.jpeg" alt="Sparse Semi-DETR">
                      </td>
                      <td>
                        <papertitle>Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object Detection</papertitle>
                        T. Shehzadi <strong>K.A. Hashmi</strong>, D.Stricker, M.Z. Afzal
                        <br>
                        <em>CVPR</em>, 2024
                        <br>
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shehzadi_Sparse_Semi-DETR_Sparse_Learnable_Queries_for_Semi-Supervised_Object_Detection_CVPR_2024_paper.pdf">pdf</a> /
                        <a href="https://www.youtube.com/watch?v=lCjCveKC8ys&ab_channel=TahiraShehzadi">video</a>
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper" style="background-color: #ffffd0">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/featenhancer.gif" alt="FeatEnHancer">
                      </td>
                      <td>
                        <papertitle>FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision</papertitle>
                        <strong>K.A. Hashmi</strong>, G. Kallempudi, D.Stricker, M.Z. Afzal
                        <br>
                        <em>ICCV</em>, 2023
                        <br>
                        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hashmi_FeatEnHancer_Enhancing_Hierarchical_Features_for_Object_Detection_and_Beyond_Under_ICCV_2023_paper.pdf">pdf</a> /
                        <a href="https://khurramhashmi.github.io/FeatEnHancer/">webpage</a> /
                        <a href="https://github.com/khurramHashmi/FeatEnHancer">code</a>
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/boxmask.gif" alt="BoxMask">
                      </td>
                      <td>
                        <papertitle>BoxMask: Revisiting Bounding Box Supervision for Video Object Detection</papertitle>
                        <strong>K.A. Hashmi</strong>, A.Pagani. D.Stricker, M.Z. Afzal
                        <br>
                        <em>WACV</em>, 2023
                        <br>
                        <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Hashmi_BoxMask_Revisiting_Bounding_Box_Supervision_for_Video_Object_Detection_WACV_2023_paper.pdf">pdf</a> /
                        <a href="https://github.com/khurramHashmi/BoxMask">code</a>
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/sparse_vod.gif" alt="SparseVOD">
                      </td>
                      <td>
                        <papertitle>Spatio-Temporal Learnable Proposals for End-to-End Video Object Detection</papertitle>
                        <strong>K.A. Hashmi</strong>, D.Stricker, M.Z. Afzal
                        <br>
                        <em>BMVC</em>, 2022
                        <br>
                        <a href="https://bmvc2022.mpi-inf.mpg.de/0018.pdf">pdf</a> /
                        <a href="https://bmvc2022.mpi-inf.mpg.de/0018_video.mp4">video</a>
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/transformer_OD_survey.png" alt="Transformer OD Survey">
                      </td>
                      <td>
                        <papertitle>Object Detection with Transformers: A Review</papertitle>
                        T. Shehzadi <strong>K.A. Hashmi</strong>, D.Stricker, M.Z. Afzal
                        <br>
                        <em>arXiv</em>, 2023
                        <br>
                        <a href="https://arxiv.org/pdf/2306.04670.pdf">pdf</a> /
                        <a href="https://github.com/mindgarage-shan/transformer_object_detection_survey">code</a>
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/att_VOD.png" alt="Attention Guided VOD">
                      </td>
                      <td>
                        <papertitle>Attention-Guided Disentangled Feature Aggregation for Video Object Detection</papertitle>
                        S. Muralidhara, <strong>K.A. Hashmi</strong>, A.Pagani, D.Stricker, M.Z. Afzal
                        <br>
                        <em>Sensors</em>, 2022
                        <br>
                        <a href="https://www.mdpi.com/1424-8220/22/21/8583/pdf?version=1668083649">pdf</a>
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/instance_challenging_env.jpg" alt="Instance Segmentation">
                      </td>
                      <td>
                        <papertitle>Exploiting Concepts of Instance Segmentation to Boost Detection in Challenging Environments</papertitle>
                        <strong>K.A. Hashmi</strong>, A.Pagani, M. Liwicki, D.Stricker, M.Z. Afzal
                        <br>
                        <em>Sensors</em>, 2022
                        <br>
                        <a href="https://www.mdpi.com/1424-8220/22/10/3703/pdf?version=1652411446">pdf</a>
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/table_str.jpg" alt="Table Structure">
                      </td>
                      <td>
                        <papertitle>Guided Table Structure Recognition Through Anchor Optimization</papertitle>
                        <strong>K.A. Hashmi</strong>, D.Stricker, M. Liwicki, M.Z. Afzal
                        <br>
                        <em>IEEE ACCESS</em>, 2022
                        <br>
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9508971">pdf</a>
                      </td>
                    </tr>
                  </table>
                </div>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="services">
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="section">Services</span>
                <p class="bio">
                  <strong>Reviewer of Conferences:</strong><br>
                  ICCV2025, CVPR2025, ICLR2024, ECCV2024, CVPR2024, WACV2023, ECCV2022, BMVC2022
                </p>
                <p class="bio">
                  <strong>Reviewer of Journals:</strong><br>
                  IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)<br>
                  IEEE Access<br>
                  Springer Nature<br>
                  Neurocomputing<br>
                  Journal of Imaging
                </p>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="section">Honors & Awards</span>
                <ul>
                  <li>Received merit based full scholarship for complete Bachelors</li>
                  <li>Nominated for the best AI Newcomer Award by German Association of Computer Science</li>
                  <li>Nominated for the AI Person of the Year Award 2024 by data:unplugged and t3n Magazin</li>
                </ul>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <p style="text-align:center;font-size:small;">
                  Template credits: <a href="https://jonbarron.info/">Jon Barron</a>.<br>
                </p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
    </table>
  </body>
</html>


